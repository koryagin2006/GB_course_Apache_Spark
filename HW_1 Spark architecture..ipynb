{
  "metadata": {
    "name": "HW_1 Spark architecture",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Урок 1. Архитектура Spark. Принципы исполнения запросов. Сохранение и чтение данных"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.sparkContext.applicationId"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Домашнее задание 1. Визуализация\n[https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv](https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv)\n\n1. Построить распределения клиентов по возрастам\n2. Распределение по возрасту с динамическим численным параметром `max_age`\n3. Распределение по возрасту с динамическим параметром `marital`"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nbank_df \u003d spark.table(\"homework.bank\")\r\nbank_df.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nz.show(bank_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(\n    bank_df.filter(bank_df[\"age\"] \u003e\u003d z.input(\"max_age\"))\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmarital_status \u003d [(\"\u0027single\u0027\",\"single\"), (\"\u0027married\u0027\",\"married\"), (\"\u0027divorced\u0027\",\"divorced\")]\nmarital_status_list \u003d \", \".join(z.checkbox(\"marital_status\", marital_status, [\"\u0027single\u0027\"]))\nsql_select \u003d \"select bank.marital, age, count(*) as count from homework.bank where bank.marital in (\" + marital_status_list + \") group by age, bank.marital\"\nz.show(spark.sql(sql_select))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Домашнее задание 2. Fire Station onboarding\n[/user/admin/sf-fire-calls.csv](/user/admin/sf-fire-calls.csv)\n1. What were all the different types of fire calls in 2018?\n2. What months within the year 2018 saw the highest number of fire calls?\n3. Which neighborhood in San Francisco generated the most fire calls in 2018?\n4. Which neighborhoods had the worst response times to fire calls in 2018?\n5. Which week in the year in 2018 had the most fire calls?\n6. Is there a correlation between neighborhood, zip code, and number of fire calls?\n7. How can we use Parquet files or SQL tables to store this data and read it back?"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npath \u003d \u0027/user/admin/sf-fire-calls.csv\u0027\n\nfire_station_df \u003d spark.read.option(\"header\", True).csv(path)\nfire_station_df.createOrReplaceTempView(\"fire_station_table\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfire_station_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    DISTINCT(CallType) as types_of_fire_calls,\n    count(UnitID) as number_of_fire_calls\nFROM fire_station_table WHERE CallDate LIKE \u0027%/2018\u0027\ngroup by types_of_fire_calls;"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT\n    SUBSTRING(CallDate, 7, 4) as year,\n    date_format(concat(SUBSTRING(CallDate, 7, 4), \u0027-\u0027, SUBSTRING(CallDate, 1, 2), \u0027-\u0027, SUBSTRING(CallDate, 4, 2)), \u0027MMM\u0027) as month,\n    -- SUBSTRING(CallDate, 1, 2) as month,\n    count(UnitID) as number_of_fire_calls\nFROM fire_station_table\nWHERE CallDate LIKE \u0027%/2018\u0027\ngroup by month, year\norder by number_of_fire_calls desc\nlimit 3;"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect Neighborhood, count(UnitID) as number_of_fire_calls\nfrom fire_station_table\nWHERE CallDate LIKE \u0027%/2018\u0027 and City \u003d \u0027San Francisco\u0027\ngroup by Neighborhood\norder by number_of_fire_calls desc\nlimit 1;"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    date_format(concat(SUBSTRING(CallDate, 7, 4), \u0027-\u0027, SUBSTRING(CallDate, 1, 2), \u0027-\u0027, SUBSTRING(CallDate, 4, 2)), \u0027w\u0027) as week,\n    count(UnitID) AS number_of_fire_calls\nFROM fire_station_table\nWHERE CallDate LIKE \u0027%/2018\u0027\nGROUP BY week\nORDER BY number_of_fire_calls DESC\nLIMIT 1;"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT\n    Neighborhood, \n    sum(Delay) as sum_delay\nFROM fire_station_table\nWHERE CallDate LIKE \u0027%/2018\u0027\nGROUP BY Neighborhood\nOREDER BY sum_delay desc\nLIMIT 8;"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT\n    Neighborhood, Zipcode, count(UnitID) AS number_of_fire_calls\nFROM fire_station_table\nGROUP BY Neighborhood, Zipcode\nORDER BY number_of_fire_calls DESC\nLIMIT 30;\n\n-- не смог сделать!"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Как мы можем использовать файлы Parquet или таблицы SQL для хранения этих данных и считывания их обратно?\n\n- Мы можем сохранить эти данные в файл parquet или таблицу hive, и работать уже не как с pandas.df, а уже как sql таблицей + исользовать тогда join разных таблиц"
    }
  ]
}